# LDM配置 - 微多普勒数据集 (256x256 → 32x32 latent)
# 针对VAE Latent空间优化的架构，适合31个用户的步态数据
# 参考Stable Diffusion等成功的Latent Diffusion Model设计

model:
  # 图像尺寸
  image_size: 256  # 原始图像尺寸（VAE会编码到32x32 latent）
  
  # 通道配置 - 为Latent空间优化的大模型
  num_channels: 256  # 增加基础通道数（适合latent空间）
  num_res_blocks: 3   # 增加残差块数量（提升模型容量）
  channel_mult: ""    # 留空，让模型根据latent_size自动选择
  
  # 注意力配置 - 适合32x32 latent空间
  attention_resolutions: [16, 8, 4]  # 在32/16=2, 32/8=4, 32/4=8分辨率使用注意力
  num_heads: 8        # 增加注意力头数（更好的全局建模）
  num_head_channels: 32  # 每个头的通道数
  
  # 其他架构选项
  dropout: 0.1
  use_scale_shift_norm: true
  resblock_updown: true
  use_new_attention_order: true
  use_checkpoint: false  # 内存充足时关闭，加快训练
  
  # VAE集成配置（新增）
  in_channels: 4   # VAE latent是4通道，不是RGB的3
  out_channels: 4  # 同样输出4通道
  learn_sigma: false  # 简化训练，不学习方差

diffusion:
  # 扩散步数
  steps: 1000
  
  # 噪声调度
  noise_schedule: cosine  # cosine对小数据集更友好
  
  # Beta schedule参数
  beta_start: 0.0001
  beta_end: 0.02
  
  # 损失类型
  loss_type: mse  # 简单MSE损失
  
  # 采样配置
  timestep_respacing: ""  # 训练时使用全部步数
  
training:
  # 批大小
  batch_size: 8   # 256x256图像需要更小的batch以适应内存
  microbatch: 2   # 梯度累积
  
  # 优化器
  lr: 1e-4
  weight_decay: 0.0
  ema_rate: 0.9999
  
  # 训练周期
  lr_anneal_steps: 0  # 不使用学习率退火
  
  # 日志和保存
  log_interval: 10
  save_interval: 1000
  
  # 混合精度
  use_fp16: false  # 初期调试时关闭，稳定后可开启
  fp16_scale_growth: 1e-3

data:
  # 数据路径
  data_dir: ""  # 将在脚本中设置
  
  # 数据增强 - 完全不使用
  random_flip: false  # 不使用水平翻转
  random_crop: false  # 不使用随机裁剪
  
  # 类别条件
  class_cond: true  # 31个用户类别
  num_classes: 31

# VAE配置（新增）
vae:
  model_path: "domain_adaptive_diffusion/vae/vae_model.pt"
  scale_factor: 0.18215  # 标准缩放因子
  
# 推理配置
sampling:
  batch_size: 4       # 生成时的批大小（256x256需要较小batch）
  num_samples: 31     # 每个类别生成一个样本
  use_ddim: true      # 默认使用DDIM快速采样
  ddim_steps: 50      # DDIM使用50步（比1000步快20倍）
  clip_denoised: true # 裁剪去噪结果
  classifier_scale: 0.0  # 不使用分类器引导

